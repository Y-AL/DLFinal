{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# models\nfrom typing import List\nimport numpy as np\nfrom torch import nn\nfrom torch.nn import functional as F\nimport torch\nfrom transformers import ViTConfig, ViTModel\n\n# schedulers\nfrom enum import auto, Enum\nimport math\n\n# normalizer\nimport torch\n\n# dataset\nfrom typing import NamedTuple, Optional\nimport torch\nimport numpy as np\n\nfrom tqdm.auto import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:03:05.292770Z","iopub.execute_input":"2024-12-12T07:03:05.293173Z","iopub.status.idle":"2024-12-12T07:03:26.688131Z","shell.execute_reply.started":"2024-12-12T07:03:05.293129Z","shell.execute_reply":"2024-12-12T07:03:26.686815Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class WallSample(NamedTuple):\n    states: torch.Tensor\n    locations: torch.Tensor\n    actions: torch.Tensor\n\n\nclass WallDataset:\n    def __init__(\n        self,\n        data_path,\n        probing=False,\n        device=\"cuda\",\n    ):\n        self.device = device\n        self.states = np.load(f\"{data_path}/states.npy\", mmap_mode=\"r\")\n        self.actions = np.load(f\"{data_path}/actions.npy\")\n\n        if probing:\n            self.locations = np.load(f\"{data_path}/locations.npy\")\n        else:\n            self.locations = None\n\n    def __len__(self):\n        return len(self.states)\n\n    def __getitem__(self, i):\n        states = torch.from_numpy(self.states[i]).float().to(self.device)\n        actions = torch.from_numpy(self.actions[i]).float().to(self.device)\n\n        if self.locations is not None:\n            locations = torch.from_numpy(self.locations[i]).float().to(self.device)\n        else:\n            locations = torch.empty(0).to(self.device)\n\n        return WallSample(states=states, locations=locations, actions=actions)\n\n\ndef create_wall_dataloader(\n    data_path,\n    probing=False,\n    device=\"cuda\",\n    batch_size=64,\n    train=True,\n):\n    ds = WallDataset(\n        data_path=data_path,\n        probing=probing,\n        device=device,\n    )\n\n    loader = torch.utils.data.DataLoader(\n        ds,\n        batch_size,\n        shuffle=train,\n        drop_last=True,\n        pin_memory=False,\n    )\n\n    return loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:03:26.690006Z","iopub.execute_input":"2024-12-12T07:03:26.690619Z","iopub.status.idle":"2024-12-12T07:03:26.701963Z","shell.execute_reply.started":"2024-12-12T07:03:26.690583Z","shell.execute_reply":"2024-12-12T07:03:26.700515Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def build_mlp(layers_dims: List[int]):\n    layers = []\n    for i in range(len(layers_dims) - 2):\n        layers.append(nn.Linear(layers_dims[i], layers_dims[i + 1]))\n        layers.append(nn.BatchNorm1d(layers_dims[i + 1]))\n        layers.append(nn.ReLU(True))\n    layers.append(nn.Linear(layers_dims[-2], layers_dims[-1]))\n    return nn.Sequential(*layers)\n\n\nclass MockModel(torch.nn.Module):\n    \"\"\"\n    Does nothing. Just for testing.\n    \"\"\"\n\n    def __init__(self, device=\"cuda\", bs=64, n_steps=17, output_dim=256):\n        super().__init__()\n        self.device = device\n        self.bs = bs\n        self.n_steps = n_steps\n        self.repr_dim = 256\n\n    def forward(self, states, actions):\n        \"\"\"\n        Args:\n            During training:\n                states: [B, T, Ch, H, W]\n            During inference:\n                states: [B, 1, Ch, H, W]\n            actions: [B, T-1, 2]\n\n        Output:\n            predictions: [B, T, D]\n        \"\"\"\n        return torch.randn((self.bs, self.n_steps, self.repr_dim)).to(self.device)\n\n\nclass Encoder_ViT(nn.Module):\n    def __init__(self, reprst_H, reprst_W):\n        super().__init__()\n        self.reprst_H = reprst_H\n        self.reprst_W = reprst_W\n         # Initializing a ViT vit-base-patch16-224 style configuration\n        self.config_ViT = ViTConfig(\n            hidden_size=self.reprst_H*self.reprst_W, # todo\n            num_hidden_layers=2, # 4\n            num_attention_heads=1, \n            intermediate_size=512, \n            image_size=65, \n            patch_size=13, \n            num_channels=2,\n            return_dict=True\n        )\n        # Initializing a model (with random weights) from the vit-base-patch16-224 style configuration\n        self.bareViT = ViTModel(self.config_ViT) \n        # The bare ViT Model transformer outputting raw hidden-states without any specific head on top. \n        \"\"\"\n        last_hidden_state (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size))\n        — Sequence of hidden-states at the output of the last layer of the model.\n        \n        pooler_output (torch.FloatTensor of shape (batch_size, hidden_size))\n        — Last layer hidden-state of the first token of the sequence (classification token) after further processing through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns the classification token after processing through a linear layer and a tanh activation function. The linear layer weights are trained from the next sentence prediction (classification) objective during pretraining.\n        \"\"\"\n\n    def forward(self, observs):\n        \"\"\"\n        Args:\n            During training:\n                observs: [B, Ch, H, W]\n            During inference: will unroll the JEPA world model recurrently into the future, conditioned on \"initial\" observation and action sequence \n                observs: [B, Ch, H, W] ??\n        Output:\n            target_states: [B, reprst_H*reprst_W] or [B, reprst_H, reprst_W]\n        \"\"\"\n        target_states = self.bareViT(observs)\n        return target_states.pooler_output # [B, hidden_size] = [B, reprst_H*reprst_W]\n\n\nclass Predictor_1dCNN(nn.Module):\n    def __init__(self, reprst_H, reprst_W):\n        super().__init__()\n        self.reprst_D = reprst_H*reprst_W\n        self.action_projector = nn.Sequential(\n            nn.Linear(2, self.reprst_D),\n            nn.ReLU()\n        )\n        # input: [B, 2, reprst_H*reprst_W]\n        self.cnn = nn.Sequential(\n            nn.Conv1d(in_channels=2, out_channels=16, kernel_size=3, padding=1), # ch1: prev_states, ch2: actions_proj\n            nn.ReLU(),\n            nn.BatchNorm1d(16),\n            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm1d(32),\n            nn.Conv1d(in_channels=32, out_channels=1, kernel_size=3, padding=1),\n            # nn.ReLU(),\n            # nn.BatchNorm2d(64),\n            # nn.Conv2d(in_channels=64, out_channels=2, kernel_size=3, padding=1)\n        )\n        # input: [B, 1, reprst_H*reprst_W]\n\n    def forward(self, prev_states, actions):\n        \"\"\"\n        Args:\n            During training:\n                prev_states: [B, reprst_H*reprst_W]\n            During inference: will unroll the JEPA world model recurrently into the future, conditioned on \"initial\" observation and action sequence \n                prev_states: [B, reprst_H*reprst_W]\n            actions: [B, 2]\n        Output:\n            curr_states: [B, reprst_H*reprst_W]\n        \"\"\"\n        actions_proj = self.action_projector(actions) # [B, reprst_H*reprst_W]\n        input = torch.stack((prev_states, actions_proj), dim=1) # input: [B, 2, reprst_H*eprst_W]\n        curr_states = self.cnn(input) # [B, 1, reprst_H*reprst_W]\n        curr_states = curr_states.view(-1, self.reprst_D) # [B, reprst_H*reprst_W]\n        return curr_states\n    \n\n\nclass Predictor_2dCNN(nn.Module):\n    def __init__(self, reprst_H, reprst_W):\n        super().__init__()\n        self.reprst_H = reprst_H\n        self.reprst_W = reprst_W\n        self.action_projector = nn.Sequential(\n            nn.Linear(2, self.reprst_H * self.reprst_W),\n            nn.ReLU()\n        )\n        # input: [B, 2, reprst_H, reprst_W]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(in_channels=2, out_channels=16, kernel_size=3, padding=1), # ch1: prev_states, ch2: actions_proj\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.Conv2d(in_channels=32, out_channels=1, kernel_size=3, padding=1),\n            # nn.ReLU(),\n            # nn.BatchNorm2d(64),\n            # nn.Conv2d(in_channels=64, out_channels=2, kernel_size=3, padding=1)\n        )\n        # input: [B, 1, reprst_H, reprst_W]\n\n    def forward(self, prev_states, actions):\n        \"\"\"\n        Args:\n            During training:\n                prev_states: [B, reprst_H, reprst_W]\n            During inference: will unroll the JEPA world model recurrently into the future, conditioned on \"initial\" observation and action sequence \n                prev_states: [B, reprst_H, reprst_W]\n            actions: [B, 2]\n        Output:\n            curr_states: [B, reprst_H, reprst_W]\n        \"\"\"\n        actions_proj = self.action_projector(actions).view(-1, self.reprst_H, self.reprst_W) # [B, reprst_H, reprst_W]\n        input = torch.stack((prev_states, actions_proj), dim=1) # input: [B, 2, reprst_H, reprst_W]\n        curr_states = self.cnn(input) # [B, 1, reprst_H, reprst_W]\n        curr_states = curr_states.view(-1, self.reprst_H, self.reprst_W) # [B, reprst_H, reprst_W]\n        return curr_states\n\n\nclass JEPAWorldModel(nn.Module):\n    def __init__(self, encoder, encoder_target, predictor, device=\"cuda\"):\n        super().__init__()\n        self.encoder = encoder # todo: same or not\n        self.encoder_target = encoder_target # todo: same or not\n        self.predictor = predictor\n        # self.funct_distance = funct_distance\n        self.device = device\n\n    def forward(self, observs, actions):\n        \"\"\"\n        Args:\n            During training:\n                observs: [B(batch size), T, Ch, H, W]\n            During inference: will unroll the JEPA world model recurrently into the future, conditioned on \"initial\" observation and action sequence \n                observs: [B, 1, Ch, H, W]\n            actions: [B, T-1, 2]\n        Output:\n            predictions: [B, T, D (\"flattened\" repr_dim)]\n            targets: \n        \"\"\"\n        Bsize, T, _, _, _ = observs.shape\n        pred_states = [] \n        target_states = []\n        \n        states_0 = self.encoder(observs[:, 0]) # states_0: [B, D], observs[:, 0]: [B, Ch, H, W]\n        pred_states_1 = self.predictor(states_0, actions[:, 0]) # pred_states_1: [B, D]\n        pred_states.append(pred_states_1) # [s1]\n        target_states_1 = self.encoder_target(observs[:, 1]) # target_states_1: [B, D]\n        target_states.append(target_states_1) # [s1']\n        \n        for t in range(1, T-1):\n            pred_states_t = self.predictor(pred_states[t-1], actions[:, t])\n            pred_states.append(pred_states_t) # [s1, s2]\n            target_states_t = self.encoder_target(observs[:, t+1])\n            target_states.append(target_states_t) # [s1', s2']\n\n        return torch.stack(pred_states, dim=1), torch.stack(target_states, dim=1) # concatenate states of different timesteps => [B, T-1, D]\n\n\nclass Prober(torch.nn.Module):\n    def __init__(\n        self,\n        embedding: int,\n        arch: str,\n        output_shape: List[int],\n    ):\n        super().__init__()\n        self.output_dim = np.prod(output_shape)\n        self.output_shape = output_shape\n        self.arch = arch\n\n        arch_list = list(map(int, arch.split(\"-\"))) if arch != \"\" else []\n        f = [embedding] + arch_list + [self.output_dim]\n        layers = []\n        for i in range(len(f) - 2):\n            layers.append(torch.nn.Linear(f[i], f[i + 1]))\n            layers.append(torch.nn.ReLU(True))\n        layers.append(torch.nn.Linear(f[-2], f[-1]))\n        self.prober = torch.nn.Sequential(*layers)\n\n    def forward(self, e):\n        output = self.prober(e)\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:19:04.687186Z","iopub.execute_input":"2024-12-12T07:19:04.687605Z","iopub.status.idle":"2024-12-12T07:19:04.717115Z","shell.execute_reply.started":"2024-12-12T07:19:04.687567Z","shell.execute_reply":"2024-12-12T07:19:04.715530Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class BarlowTwinsLoss(nn.Module):\n    def __init__(self, lambda_=5e-3):\n        \"\"\"\n        Barlow Twins Loss Module.\n\n        Args:\n            lambda_ (float): Scaling factor for the redundancy reduction term.\n        \"\"\"\n        super(BarlowTwinsLoss, self).__init__()\n        self.lambda_ = lambda_\n\n    def forward(self, preds, targets):\n        \"\"\"\n        Computes the Barlow Twins loss.\n\n        Args:\n            preds (torch.Tensor): Embeddings from the first view. Shape: (batch_size, T-1, embedding_dim).\n            targets (torch.Tensor): Embeddings from the second view. Shape: (batch_size, T-1, embedding_dim).\n\n        Returns:\n            torch.tensor(np.mean(lt_loss))\n        \"\"\"\n        batch_size, traj_length, embedding_dim = preds.shape\n        total_loss = 0.0\n        # lt_loss = []\n        for t in range(traj_length):\n            z1 = preds[:, t] # [batch_size, embedding_dim]\n            z2 = preds[:, t]\n        \n            # Normalize embeddings\n            z1 = F.normalize(z1, dim=1)\n            z2 = F.normalize(z2, dim=1)\n            print(\"z1\", z1)\n            \n            # Cross-correlation matrix\n            # batch_size = z1.size(0)\n            c = (z1.T @ z2) / batch_size\n            print(c)\n\n            # Diagonal loss (invariance loss)\n            identity_loss = torch.mean((torch.diag(c) - 1) ** 2)\n            print(\"identity_loss: \", identity_loss)\n    \n            # Off-diagonal loss (redundancy reduction)\n            off_diag = c - torch.eye(embedding_dim, device=c.device)\n            off_diag_loss = torch.mean(off_diag ** 2)\n            print(\"off_diag_loss: \", off_diag_loss)\n\n            # Combined loss for this timestep\n            timestep_loss = identity_loss + self.lambda_ * off_diag_loss\n            print(\"timestep_loss: \", timestep_loss)\n            total_loss += timestep_loss\n    \n            # # Identity matrix\n            # on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()  # (Cii - 1)^2\n            # off_diag = self.off_diagonal(c).pow_(2).sum()       # Cij^2 for i != j\n    \n            # # Total loss\n            # loss = on_diag + self.lambda_ * off_diag\n            # lt_loss.append(loss.item())\n        return total_loss/traj_length\n\n    # @staticmethod\n    # def off_diagonal(x):\n    #     \"\"\"\n    #     Extracts the off-diagonal elements of a square matrix.\n\n    #     Args:\n    #         x (torch.Tensor): Input square matrix. Shape: (embedding_dim, embedding_dim).\n\n    #     Returns:\n    #         torch.Tensor: Flattened off-diagonal elements.\n    #     \"\"\"\n    #     n, _ = x.size()\n    #     return x.flatten()[1:].view(n - 1, n + 1)[:, :-1].flatten()\n\nclass VICRegLoss(nn.Module):\n    def __init__(self, lambda_=1e-2):\n        super().__init__()\n        self.lambda_ = lambda_\n\n    def forward(self, predicted_states, target_states):\n        predicted_states, target_states = predicted_states.to(device), target_states.to(device)\n        # 1. Prediction Loss: Minimize distance between predicted and target states\n        # pred_loss = F.mse_loss(torch.stack(predicted_states), torch.stack(target_states))\n        pred_loss = F.mse_loss(predicted_states, target_states)\n        \n        # 2. Variance Loss: Encourage representations to have non-zero variance\n        std_loss = self.variance_loss(predicted_states)\n        \n        # 3. Covariance Loss: Decorrelate representation dimensions\n        cov_loss = self.covariance_loss(predicted_states)\n        \n        # Weighted combination of losses\n        total_loss = pred_loss + 1e-2 * (std_loss + cov_loss)\n        return total_loss\n\n    def variance_loss(self, representations, min_std=0.1):\n        \"\"\"Encourage each feature to have non-zero variance\"\"\"\n        # repr_tensor = torch.stack(representations)\n        representations = representations.to(device)\n        std_loss = torch.relu(min_std - representations.std(dim=0)).mean()\n        return std_loss\n    \n    def covariance_loss(self, representations):\n        \"\"\"Decorrelate representation dimensions\"\"\"\n        # repr_tensor = torch.stack(representations)\n        representations = representations.to(device)\n        repr_tensor = representations\n        repr_tensor = repr_tensor.to(device)\n        \n        # Center the representations\n        repr_tensor = repr_tensor - repr_tensor.mean(dim=0)\n        \n        # Flatten tensor (keep batch dimension intact)\n        repr_tensor = repr_tensor.view(repr_tensor.shape[0], -1)\n        \n        # Compute covariance matrix\n        cov_matrix = (repr_tensor.T @ repr_tensor) / (repr_tensor.shape[0] - 1)\n        \n        # Decorrelate dimensions (set diagonal to zero)\n        cov_matrix.fill_diagonal_(0)\n        \n        # Compute loss\n        cov_loss = (cov_matrix ** 2).sum()\n        return cov_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:19:06.552563Z","iopub.execute_input":"2024-12-12T07:19:06.553105Z","iopub.status.idle":"2024-12-12T07:19:06.568605Z","shell.execute_reply.started":"2024-12-12T07:19:06.553065Z","shell.execute_reply":"2024-12-12T07:19:06.567489Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nreprst_H=16\nreprst_W=16\nEnc = Encoder_ViT(reprst_H, reprst_W).to(device)\nEnc_t = Encoder_ViT(reprst_H, reprst_W).to(device)\nPred = Predictor_1dCNN(reprst_H, reprst_W).to(device)\nmodel = JEPAWorldModel(encoder=Enc, encoder_target=Enc_t, predictor=Pred).to(device)\n# model.load_state_dict(torch.load(\"/content/best_model (1).pth\", weights_only=True))\n\n# criterion = BarlowTwinsLoss()\ncriterion = VICRegLoss()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer=optimizer, T_0=30, eta_min=1e-6)\n\ndataset = create_wall_dataloader(data_path='./DL24FA/train', device=device, batch_size=64)\n\nnum_epochs = 10\nmin_loss = float('inf')\n# step = 0\nfor epoch in tqdm(range(num_epochs), desc=f\"\"):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(dataset, desc=\"\"):\n        observs = batch.states.to(device)\n        actions = batch.actions.to(device)\n        \n        optimizer.zero_grad()\n        \n        pred_states, target_states = model(observs, actions)\n        loss = criterion(pred_states, target_states) # mean of losses (across timesteps)\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        \n        # scheduler step\n\n        # if step % 100 == 0:\n        #     print(f\"training loss {loss.item()}\")\n\n        # step += 1\n    mean_loss = total_loss/len(dataset)\n    print(f\"Epoch: {epoch+1}, Training Loss: {mean_loss: .4f}\")\n\n    if mean_loss < min_loss:\n        min_loss = mean_loss\n        torch.save(model.state_dict(), \"best_ViT_1dCNN.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}