{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import NamedTuple, Optional\nimport torch\nimport numpy as np\n\n\nclass WallSample(NamedTuple):\n    states: torch.Tensor\n    locations: torch.Tensor\n    actions: torch.Tensor\n\n\nclass WallDataset:\n    def __init__(\n        self,\n        data_path,\n        probing=False,\n        device=\"cuda\",\n    ):\n        self.device = device\n        self.states = np.load(f\"{data_path}/states.npy\", mmap_mode=\"r\")\n        self.actions = np.load(f\"{data_path}/actions.npy\")\n\n        if probing:\n            self.locations = np.load(f\"{data_path}/locations.npy\")\n        else:\n            self.locations = None\n\n    def __len__(self):\n        return len(self.states)\n\n    def __getitem__(self, i):\n        states = torch.from_numpy(self.states[i]).float().to(self.device)\n        actions = torch.from_numpy(self.actions[i]).float().to(self.device)\n\n        if self.locations is not None:\n            locations = torch.from_numpy(self.locations[i]).float().to(self.device)\n        else:\n            locations = torch.empty(0).to(self.device)\n\n        return WallSample(states=states, locations=locations, actions=actions)\n\n\ndef create_wall_dataloader(\n    data_path,\n    probing=False,\n    device=\"cuda\",\n    batch_size=64,\n    train=True,\n):\n    ds = WallDataset(\n        data_path=data_path,\n        probing=probing,\n        device=device,\n    )\n\n    loader = torch.utils.data.DataLoader(\n        ds,\n        batch_size,\n        shuffle=train,\n        drop_last=True,\n        pin_memory=False,\n    )\n\n    return loader\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import NamedTuple, List, Any, Optional, Dict\nfrom itertools import chain\nfrom dataclasses import dataclass\nimport itertools\nimport os\nimport torch\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom schedulers import Scheduler, LRSchedule\nfrom models import Prober, build_mlp\nfrom configs import ConfigBase\n\nfrom dataset import WallDataset\nfrom normalizer import Normalizer\n\n\n@dataclass\nclass ProbingConfig(ConfigBase):\n    probe_targets: str = \"locations\"\n    lr: float = 0.00002\n    epochs: int = 20\n    schedule: LRSchedule = LRSchedule.Cosine\n    sample_timesteps: int = 30\n    prober_arch: str = \"256\"\n\n\nclass ProbeResult(NamedTuple):\n    model: torch.nn.Module\n    average_eval_loss: float\n    eval_losses_per_step: List[float]\n    plots: List[Any]\n\n\ndefault_config = ProbingConfig()\n\n\ndef location_losses(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n    assert pred.shape == target.shape\n    mse = (pred - target).pow(2).mean(dim=0)\n    return mse\n\n\nclass ProbingEvaluator:\n    def __init__(\n        self,\n        device: \"cuda\",\n        model: torch.nn.Module,\n        probe_train_ds,\n        probe_val_ds: dict,\n        config: ProbingConfig = default_config,\n        quick_debug: bool = False,\n    ):\n        self.device = device\n        self.config = config\n\n        self.model = model\n        self.model.eval()\n\n        self.quick_debug = quick_debug\n\n        self.ds = probe_train_ds\n        self.val_ds = probe_val_ds\n\n        self.normalizer = Normalizer()\n\n    def train_pred_prober(self):\n        \"\"\"\n        Probes whether the predicted embeddings capture the future locations\n        \"\"\"\n        repr_dim = self.model.repr_dim\n        dataset = self.ds\n        model = self.model\n\n        config = self.config\n        epochs = config.epochs\n\n        if self.quick_debug:\n            epochs = 1\n        test_batch = next(iter(dataset))\n\n        prober_output_shape = getattr(test_batch, \"locations\")[0, 0].shape\n        prober = Prober(\n            repr_dim,\n            config.prober_arch,\n            output_shape=prober_output_shape,\n        ).to(self.device)\n\n        all_parameters = []\n        all_parameters += list(prober.parameters())\n\n        optimizer_pred_prober = torch.optim.Adam(all_parameters, config.lr)\n\n        step = 0\n\n        batch_size = dataset.batch_size\n        batch_steps = None\n\n        scheduler = Scheduler(\n            schedule=self.config.schedule,\n            base_lr=config.lr,\n            data_loader=dataset,\n            epochs=epochs,\n            optimizer=optimizer_pred_prober,\n            batch_steps=batch_steps,\n            batch_size=batch_size,\n        )\n\n        for epoch in tqdm(range(epochs), desc=f\"Probe prediction epochs\"):\n            for batch in tqdm(dataset, desc=\"Probe prediction step\"):\n                ################################################################################\n                # TODO: Forward pass through your model\n                init_states = batch.states[:, 0:1]  # BS, 1, C, H, W\n                pred_encs, _ = model(init_states, batch.actions)\n                pred_encs = pred_encs.transpose(0, 1)  # # BS, T, D --> T, BS, D\n\n                # Make sure pred_encs has shape (T, BS, D) at this point\n                ################################################################################\n\n                pred_encs = pred_encs.detach()\n\n                n_steps = pred_encs.shape[0]\n                bs = pred_encs.shape[1]\n\n                losses_list = []\n\n                target = getattr(batch, \"locations\").cuda()\n                target = self.normalizer.normalize_location(target)\n\n                if (\n                    config.sample_timesteps is not None\n                    and config.sample_timesteps < n_steps\n                ):\n                    sample_shape = (config.sample_timesteps,) + pred_encs.shape[1:]\n                    # we only randomly sample n timesteps to train prober.\n                    # we most likely do this to avoid OOM\n                    sampled_pred_encs = torch.empty(\n                        sample_shape,\n                        dtype=pred_encs.dtype,\n                        device=pred_encs.device,\n                    )\n\n                    sampled_target_locs = torch.empty(bs, config.sample_timesteps, 2)\n\n                    for i in range(bs):\n                        indices = torch.randperm(n_steps)[: config.sample_timesteps]\n                        sampled_pred_encs[:, i, :] = pred_encs[indices, i, :]\n                        sampled_target_locs[i, :] = target[i, indices]\n\n                    pred_encs = sampled_pred_encs\n                    target = sampled_target_locs.cuda()\n\n                pred_locs = torch.stack([prober(x) for x in pred_encs], dim=1)\n                losses = location_losses(pred_locs, target)\n                per_probe_loss = losses.mean()\n\n                if step % 100 == 0:\n                    print(f\"normalized pred locations loss {per_probe_loss.item()}\")\n\n                losses_list.append(per_probe_loss)\n                optimizer_pred_prober.zero_grad()\n                loss = sum(losses_list)\n                loss.backward()\n                optimizer_pred_prober.step()\n\n                lr = scheduler.adjust_learning_rate(step)\n\n                step += 1\n\n                if self.quick_debug and step > 2:\n                    break\n\n        return prober\n\n    @torch.no_grad()\n    def evaluate_all(\n        self,\n        prober,\n    ):\n        \"\"\"\n        Evaluates on all the different validation datasets\n        \"\"\"\n        avg_losses = {}\n\n        for prefix, val_ds in self.val_ds.items():\n            avg_losses[prefix] = self.evaluate_pred_prober(\n                prober=prober,\n                val_ds=val_ds,\n                prefix=prefix,\n            )\n\n        return avg_losses\n\n    @torch.no_grad()\n    def evaluate_pred_prober(\n        self,\n        prober,\n        val_ds,\n        prefix=\"\",\n    ):\n        quick_debug = self.quick_debug\n        config = self.config\n\n        model = self.model\n        probing_losses = []\n        prober.eval()\n\n        for idx, batch in enumerate(tqdm(val_ds, desc=\"Eval probe pred\")):\n            ################################################################################\n            # TODO: Forward pass through your model\n            init_states = batch.states[:, 0:1]  # BS, 1 C, H, W\n            pred_encs, _ = model(init_states, batch.actions)\n            # # BS, T, D --> T, BS, D\n            pred_encs = pred_encs.transpose(0, 1)\n\n            # Make sure pred_encs has shape (T, BS, D) at this point\n            ################################################################################\n\n            target = getattr(batch, \"locations\").cuda()\n            target = self.normalizer.normalize_location(target)\n\n            pred_locs = torch.stack([prober(x) for x in pred_encs], dim=1)\n            losses = location_losses(pred_locs, target)\n            probing_losses.append(losses.cpu())\n\n        losses_t = torch.stack(probing_losses, dim=0).mean(dim=0)\n        losses_t = self.normalizer.unnormalize_mse(losses_t)\n\n        losses_t = losses_t.mean(dim=-1)\n        average_eval_loss = losses_t.mean().item()\n\n        return average_eval_loss\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nimport torch.nn as nn\nimport math\nimport torch.nn.functional as F\nimport copy\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nfrom copy import deepcopy\nimport torchvision.models as models\nfrom torchvision.models.resnet import BasicBlock\n\n\nclass CNNEncoder(nn.Module):\n    def __init__(self, embedding_dim=64):\n        super().__init__()\n        self.in_channels = 64\n        \n        self.conv1 = nn.Conv2d(2, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=7, stride=3, padding=1)\n        \n        # Residual layers\n        self.layer1 = self._make_layer(BasicBlock, 64, 1)  # Single block\n        \n        # # Adaptive pooling and fully connected layer\n        # self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(6400, embedding_dim)\n    \n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels),\n            )\n        \n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels\n        for _ in range(1, blocks):\n            layers.append(block(out_channels, out_channels))\n    \n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        \n        x = self.layer1(x)\n        # x = self.layer2(x)\n        # x = self.layer3(x)\n        # x = self.layer4(x)\n        \n        # x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n\nclass Predictor(nn.Module):\n    def __init__(self, representation_dim=64, action_dim=2):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(representation_dim + action_dim, representation_dim),\n            nn.ReLU(),\n            nn.Linear(representation_dim, representation_dim)\n        )\n    \n    def forward(self, prev_rep, action):\n        prev_rep, action = prev_rep, action\n        # Concatenate previous representation and action\n        input_combined = torch.cat([prev_rep, action], dim=-1)\n        return self.network(input_combined)\n\n\nclass JEPAWorldModel(nn.Module):\n    \"\"\"\n    Joint Embedding Predictive Architecture World Model with ViT\n    \"\"\"\n    def __init__(self, device, representation_dim=64, action_dim=2, training=False):\n        super().__init__()\n        self.encoder = CNNEncoder()\n        self.predictor = Predictor(representation_dim, action_dim)\n        \n        # Use same encoder for target encoder (similar to VicReg)\n        self.target_encoder = deepcopy(self.encoder)\n        \n        # Synchronize target encoder with main encoder\n        self.update_target_encoder()\n        self.repr_dim = representation_dim\n        self.device = device\n        self.training = training\n    \n    def update_target_encoder(self, tau=0.995):\n        \"\"\"\n        Exponential Moving Average (EMA) update of target encoder\n        \"\"\"\n        for param_q, param_k in zip(self.encoder.parameters(), self.target_encoder.parameters()):\n            param_k.data = param_k.data * tau + param_q.data * (1. - tau)\n\n\n    def forward(self, observations, actions):\n        # Move observations and actions to device\n        # return self._reshape(observations, actions), None\n        observations, actions = observations, actions\n                \n        # Encode all observations at once using the encoder\n        # encoded_all_states = self.encoder(observations.view(-1, *observations.shape[2:]))\n        batch_size, seq_len, channels, height, width = observations.shape\n        flat_observations = observations.view(-1, channels, height, width)\n        encoded_all_states = self.encoder(flat_observations)\n        encoded_all_states = encoded_all_states.view(*observations.shape[:2], -1)  # Reshape back to (batch, sequence, features)\n        \n        # Initialize storage for predicted and target states\n        predicted_states = []\n        target_states = []\n    \n        # Shift actions to align with the sequence (actions at t predict state at t+1)\n        prev_states = encoded_all_states[:, :-1]  # Remove the last state\n        next_states = encoded_all_states[:, 1:]   # Remove the first state\n        # curr_actions = actions[:, :-1]           # Align actions with prediction        \n        \n        # Encode target states with target encoder\n        \n        with torch.no_grad():\n            target_states = self.target_encoder(flat_observations)  # Skip the first observation for alignment\n            target_states = target_states.view(*observations.shape[:2], -1)\n            target_states = target_states[:, 1:]\n            target_states = target_states\n        # Predict future representations in parallel\n        if not self.training:\n            predicted_states = self._reshape(observations, actions)\n        else:\n            predicted_states = self.predictor(prev_states, actions)\n        return predicted_states, target_states\n    \n    def compute_loss(self, predicted_states, target_states):\n        \"\"\"\n        Multi-objective loss to prevent representation collapse\n        \"\"\"\n        predicted_states, target_states = predicted_states, target_states\n        # 1. Prediction Loss: Minimize distance between predicted and target states\n        # pred_loss = F.mse_loss(torch.stack(predicted_states), torch.stack(target_states))\n        pred_loss = F.mse_loss(predicted_states, target_states)\n        \n        # 2. Variance Loss: Encourage representations to have non-zero variance\n        std_loss = self.variance_loss(predicted_states)\n        \n        # 3. Covariance Loss: Decorrelate representation dimensions\n        cov_loss = self.covariance_loss(predicted_states)\n        \n        # Weighted combination of losses\n        total_loss = pred_loss + 1e-3 * (std_loss + cov_loss)\n        return total_loss\n    \n    def variance_loss(self, representations, min_std=0.1):\n        \"\"\"Encourage each feature to have non-zero variance\"\"\"\n        # repr_tensor = torch.stack(representations)\n        representations = representations\n        std_loss = torch.relu(min_std - representations.std(dim=0)).mean()\n        return std_loss\n    \n    def covariance_loss(self, representations):\n        \"\"\"Decorrelate representation dimensions\"\"\"\n        # repr_tensor = torch.stack(representations)\n        representations = representations\n        repr_tensor = representations\n        repr_tensor = repr_tensor\n        \n        # Center the representations\n        repr_tensor = repr_tensor - repr_tensor.mean(dim=0)\n        \n        # Flatten tensor (keep batch dimension intact)\n        repr_tensor = repr_tensor.view(repr_tensor.shape[0], -1)\n        \n        # Compute covariance matrix\n        cov_matrix = (repr_tensor.T @ repr_tensor) / (repr_tensor.shape[0] - 1)\n        \n        # Decorrelate dimensions (set diagonal to zero)\n        cov_matrix.fill_diagonal_(0)\n        \n        # Compute loss\n        cov_loss = (cov_matrix ** 2).sum()\n        return cov_loss\n\n    def _get_next(self, tensor1, tensor2):\n        # Step 1: Slice tensor1 to exclude the first element along dimension=1\n        tensor1_sliced = tensor1\n        if self.training:\n            tensor1_sliced = tensor1[:, 1:, :]  # Shape [64, 16, 2]\n        \n        # Step 2: Add tensor1_sliced and tensor2\n        result_sliced = tensor1_sliced + tensor2  # Shape [64, 16, 2]\n        \n        # Step 3: Insert the first element back along dimension=1\n        result = result_sliced\n        if self.training:\n            first_element = tensor1[:, :1, :]  # Shape [64, 1, 2]\n            result = torch.cat([first_element, result_sliced], dim=1)  # Shape [64, 17, 2]\n        return result\n\n    def _reshape(self, observations, actions):\n        ans = None\n        if not self.training:\n            for i in range(actions.shape[1]):\n                action = actions[:, i:i+1]\n                channel_1 = observations[:, :, 0, :, :]\n                channel_1 = channel_1.squeeze(1)\n                # max_indices = channel_1.view(64, 17, -1).argmax(dim=-1)\n                max_indices = channel_1.view(observations.shape[0], -1).argmax(dim=-1)\n                max_coords = torch.stack([max_indices % 65, max_indices // 65], dim=-1)\n                max_coords = max_coords.unsqueeze(1)\n                if ans is None:\n                    ans = torch.cat((max_coords, torch.zeros(max_coords.shape[0], max_coords.shape[1],\n                                                             self.repr_dim - max_coords.shape[2]).to(self.device)), dim=2)\n                next_coords = self._get_next(max_coords, action)\n                next_coords = torch.cat((next_coords, \n                                         torch.zeros(next_coords.shape[0], next_coords.shape[1],\n                                                     self.repr_dim - next_coords.shape[2]).to(self.device)), dim=2)\n                ans = torch.cat([ans, next_coords], dim=1)\n            return ans\n        else:\n            channel_1 = observations[:, :, 0, :, :]\n            # max_indices = channel_1.view(64, 17, -1).argmax(dim=-1)\n            max_indices = channel_1.view(observations.shape[0], observations.shape[1], -1).argmax(dim=-1)\n            max_coords = torch.stack([max_indices % 65, max_indices // 65], dim=-1)\n            next_coords = self._get_next(max_coords, actions)\n            next_coords = torch.cat((next_coords, torch.zeros(next_coords.shape[0], next_coords.shape[1],\n                                                        self.repr_dim - next_coords.shape[2]).to(self.device)), dim=2)\n            return next_coords\n        \n        return next_coords\n\nclass DataTransforms:\n    \"\"\"\n    Image augmentations and preprocessing for JEPA training\n    \"\"\"\n    @staticmethod\n    def get_train_transforms():\n        return transforms.Compose([\n            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                                 std=[0.229, 0.224, 0.225])\n        ])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from dataset import create_wall_dataloader\n# from evaluator import ProbingEvaluator\n# import torch\n# from models import MockModel\nimport glob\n# from src.models.new_model import JEPAWorldModel\n\n\ndef get_device():\n    \"\"\"Check for GPU availability.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Using device:\", device)\n    return device\n\n\ndef load_data(device):\n    data_path = \"/scratch/DL24FA\"\n\n    probe_train_ds = create_wall_dataloader(\n        data_path=f\"{data_path}/probe_normal/train\",\n        probing=True,\n        device=device,\n        train=True,\n    )\n\n    probe_val_normal_ds = create_wall_dataloader(\n        data_path=f\"{data_path}/probe_normal/val\",\n        probing=True,\n        device=device,\n        train=False,\n    )\n\n    probe_val_wall_ds = create_wall_dataloader(\n        data_path=f\"{data_path}/probe_wall/val\",\n        probing=True,\n        device=device,\n        train=False,\n    )\n\n    probe_val_wall_other_ds = create_wall_dataloader(\n        data_path=f\"{data_path}/probe_wall_other/val\",\n        probing=True,\n        device=device,\n        train=False,\n    )\n\n    probe_val_ds = {\n        \"normal\": probe_val_normal_ds,\n        \"wall\": probe_val_wall_ds,\n        \"wall_other\": probe_val_wall_other_ds,\n    }\n\n    return probe_train_ds, probe_val_ds\n\n\ndef load_expert_data(device):\n    data_path = \"/scratch/DL24FA\"\n\n    probe_train_expert_ds = create_wall_dataloader(\n        data_path=f\"{data_path}/probe_expert/train\",\n        probing=True,\n        device=device,\n        train=True,\n    )\n\n    probe_val_expert_ds = {\n        \"expert\": create_wall_dataloader(\n            data_path=f\"{data_path}/probe_expert/val\",\n            probing=True,\n            device=device,\n            train=False,\n        )\n    }\n\n    return probe_train_expert_ds, probe_val_expert_ds\n\n\ndef seed_everything(seed: int):\n    import random, os\n    import numpy as np\n    import torch\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n\ndef load_model():\n    \"\"\"Load or initialize the model.\"\"\"\n    # TODO: Replace MockModel with your trained model\n    model = JEPAWorldModel(device=get_device(), representation_dim=64, action_dim=2).to(get_device())\n    model.load_state_dict(torch.load(\"./weights/jepa_world_model_cnn.pth\"))\n    # model.load_state_dict(torch.load(\"./latest_repo/DL_Final_Proj/weights/jepa_world_model_cnn.pth\"))\n    seed_everything(40)\n    return model\n\n\ndef evaluate_model(device, model, probe_train_ds, probe_val_ds):\n    evaluator = ProbingEvaluator(\n        device=device,\n        model=model,\n        probe_train_ds=probe_train_ds,\n        probe_val_ds=probe_val_ds,\n        quick_debug=False,\n    )\n\n    prober = evaluator.train_pred_prober()\n\n    avg_losses = evaluator.evaluate_all(prober=prober)\n\n    for probe_attr, loss in avg_losses.items():\n        print(f\"{probe_attr} loss: {loss}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = get_device()\nmodel = load_model()\n\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total Trainable Parameters: {total_params:,}\")\n\nprobe_train_ds, probe_val_ds = load_data(device)\nevaluate_model(device, model, probe_train_ds, probe_val_ds)\n\nprobe_train_expert_ds, probe_val_expert_ds = load_expert_data(device)\nevaluate_model(device, model, probe_train_expert_ds, probe_val_expert_ds)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}